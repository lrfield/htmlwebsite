### Control Systems:
---
Content serving algorithms are fundamentally closed loop control systems. User interaction and data is measured then inputted into a recommendation algorithm, which outputs content that the user then interacts with, closing the loop. The difference between being served the correct youtube videos and getting "I Have No Mouth and I Must Scream"-ed is the nature of the feedback collected and the technique used to process that feedback. I will rank some of these by how much they make me want to burn my social security card and establish an appalachain commune:
- **Explicit Feedback:** 
*"why don't you just tell me how much you liked that video?"*

Clicking the like button, subscribing, commenting, etc... This is an acceptable way to do things, but doesn't race to curation nearly as quickly as other methods.

- **Implicit Feedback:**
*"You watched the whole video, I'm gonna assume you liked it"*

Measure things that the user does not intentionally offer in order to infer their preferences. Usually behavior such as interaction frequency, scroll patterns, time spent on a page. This is getting a little dicier, you are losing control of your preferences. We are entering the territory of interfacing directly with your subconcious. You may *think* you need something more intellectually stimulating than family guy funny moments #32, but the machine *knows* that you will watch that shit
- **Collaborative Filtering:**
*"That other guy watched the same video, and he liked this other video, so I'm gonna serve you that other video!"*

Whats wrong with that? Despite mimicing the natural method of building communities around consumption preferences, the sophistication of these algorithms adds a nasty new dimension to your online profile. 
Consider the inverse of this operation. You can now be viewed in clouds of behavior with similar peers, accelerating the ablility to mine the tools most likely to manipulate you. Even if you haven't been online since 2015, it only takes a short period to associate your behavior profile with the collaborative database. BOOM! You are now subject to the "super-bug" tactics that have festered through the interaction with all of your behavioral peers in the time you've been gone.
- **Matrix Factorization:**
*"Your user matrix, multiplied by this videos interaction matrix... 32. Past acceptable recommendation score. Consume the video now"*

Second to only "Nothing to Hide, nothing to fear" the most bullshit counterargument to data collection paranoia is "Yeah, but who is going to look through all that stuff." Which, obviously, no one. They don't have to. All of your behavior online is decomposed into various black-box user interaction matrices that can be slammed into any number of algorithm to address every single nuanced latent preference factor that no human could ever recognize. But, fortunately, at the end of the day, creating these matrices relies on a "score" to minimize prediction error against. The score needs to be a quantifiable value or combination of values like watch time, interaction frequency etc... And thank god there isn't a way to mass convert subjective behaviors like emotion, political opinion, or addiction coefficient into quantifiable scores...

-**Sentiment Analysis:**
*"It's so over..."*

The final weakness of matrix factorization, and the reason it still produces a relatively "democratic" internet, is that we don't have enough knobs on it. But no longer! Now both sides of the box can be gooey, unmeasurable human vectors! 
*"Computer, I want people to like me more, recommend videos on my platform in a way that makes them like me more."*
*"Jarvis! Too many people on tik tok are calling Xi Jinping "Dictator Pooh Bear" Collate the accounts of those most likely to dissent, and change their for you page to make them suicidal!"*

-**Liquid You:**
*"Your digital anolog watched this sequence of videos in the torture cube. Syncing your information now."*

The previous entries were real algorithms and data processing tools. This item is the product of a man with a tenuous grasp on the limits of artificial intellogence spiraling into paranoia. *Stick with me here*. Instead of collecting your data and testing it on you, what if I invested a good amount of time and money into generating a simulated version of you. Of your behavior online, your preferences, click rates, opinions, what makes you angry, what makes you updoot, what makes you buy amazon products. Once I have "liquid you" the only limit I have to the rate of training my manipulation models is processing power. It would be like having the data of someone continually using a platform for 500 years. Maybe we'll get to the point where you can just open your phone and get hypno-toad-ed with a couple of flashing lights and walk willing and convinced into the Amazon-Google Fufillment center workcamp
